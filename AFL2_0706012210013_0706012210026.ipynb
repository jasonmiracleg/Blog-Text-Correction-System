{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc81f19f",
   "metadata": {},
   "source": [
    "# **Background Problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422e889d",
   "metadata": {},
   "source": [
    "# **Resource**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67640c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the years thereafter, most of the Oil fields and platforms were named after pagan “gods”.\n",
      "We love you Mr.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "# Load Dataset\n",
    "with open(\"en_US.blogs.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    \n",
    "# Split into sentences using regex\n",
    "sentences = re.split(r'(?<=[.!?]) +', text)\n",
    "\n",
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b128a99",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b4c8933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jasonmiracle/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first ten words in the text are: \n",
      "['in', 'the', 'years', 'thereafter', 'most', 'of', 'the', 'oil', 'fields', 'and']\n",
      "There are 435895 unique words in the vocabulary.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def preprocessing(text):\n",
    "    # Lower Text\n",
    "    text_lowercase = text.lower()\n",
    "\n",
    "    # Remove Punctuation\n",
    "    text_cleaned = re.sub(r'[^\\w\\s]', '', text_lowercase)  # removes punctuation\n",
    "\n",
    "    words = word_tokenize(text_cleaned)\n",
    "    \n",
    "    return words\n",
    "\n",
    "words = preprocessing(text)\n",
    "\n",
    "vocabulary = set(words)\n",
    "\n",
    "print(f\"The first ten words in the text are: \\n{words[0:10]}\")\n",
    "print(f\"There are {len(vocabulary)} unique words in the vocabulary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2884c275",
   "metadata": {},
   "source": [
    "# **Building Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75d68b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 435895 key values pairs\n",
      "The count for the word 'i' is 769059\n"
     ]
    }
   ],
   "source": [
    "# Create Dictionary of Frequency\n",
    "word_count_dict = {}\n",
    "\n",
    "for word in words:\n",
    "    word_count_dict[word] = word_count_dict.get(word, 0) + 1\n",
    "    \n",
    "print(f\"There are {len(word_count_dict)} key values pairs\")\n",
    "print(f\"The count for the word 'i' is {word_count_dict.get('i',0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb8f80c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of probs is 435895\n",
      "P('i') is 0.0207\n"
     ]
    }
   ],
   "source": [
    "# Probability Computation Function\n",
    "def count_probability(word_count_dict):\n",
    "    probability = {}\n",
    "    \n",
    "    # Count Total Words in Corpus\n",
    "    total_words = sum(word_count_dict.values())\n",
    "    \n",
    "    for key, values in word_count_dict.items():\n",
    "        probability[key] = values/total_words\n",
    "        \n",
    "    return probability\n",
    "\n",
    "probabilities = count_probability(word_count_dict)\n",
    "print(f\"Length of probs is {len(probabilities)}\")\n",
    "print(f\"P('i') is {probabilities['i']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493084b2",
   "metadata": {},
   "source": [
    "## **Editing String**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32a69fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delete suggestion : ['own', 'dwn', 'don', 'dow']\n"
     ]
    }
   ],
   "source": [
    "# Delete Function\n",
    "def delete_letter(word, verbose=False):\n",
    "    split_letter = []\n",
    "    delete_letter = []\n",
    "    \n",
    "    # Split Word Each Position\n",
    "    split_letter = [(word[:i], word[i:]) for i in range(len(word))]\n",
    "    \n",
    "    # Delete Letter Each Position\n",
    "    delete_letter = [left + right[1:] for left, right in split_letter if right]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Delete suggestion : {delete_letter}\")\n",
    "    \n",
    "    return delete_letter\n",
    "\n",
    "result = delete_letter(\"down\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a13a28d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert suggestion : ['adown', 'bdown', 'cdown', 'ddown', 'edown', 'fdown', 'gdown', 'hdown', 'idown', 'jdown', 'kdown', 'ldown', 'mdown', 'ndown', 'odown', 'pdown', 'qdown', 'rdown', 'sdown', 'tdown', 'udown', 'vdown', 'wdown', 'xdown', 'ydown', 'zdown', 'daown', 'dbown', 'dcown', 'ddown', 'deown', 'dfown', 'dgown', 'dhown', 'diown', 'djown', 'dkown', 'dlown', 'dmown', 'dnown', 'doown', 'dpown', 'dqown', 'drown', 'dsown', 'dtown', 'duown', 'dvown', 'dwown', 'dxown', 'dyown', 'dzown', 'doawn', 'dobwn', 'docwn', 'dodwn', 'doewn', 'dofwn', 'dogwn', 'dohwn', 'doiwn', 'dojwn', 'dokwn', 'dolwn', 'domwn', 'donwn', 'doown', 'dopwn', 'doqwn', 'dorwn', 'doswn', 'dotwn', 'douwn', 'dovwn', 'dowwn', 'doxwn', 'doywn', 'dozwn', 'dowan', 'dowbn', 'dowcn', 'dowdn', 'dowen', 'dowfn', 'dowgn', 'dowhn', 'dowin', 'dowjn', 'dowkn', 'dowln', 'dowmn', 'downn', 'dowon', 'dowpn', 'dowqn', 'dowrn', 'dowsn', 'dowtn', 'dowun', 'dowvn', 'dowwn', 'dowxn', 'dowyn', 'dowzn', 'downa', 'downb', 'downc', 'downd', 'downe', 'downf', 'downg', 'downh', 'downi', 'downj', 'downk', 'downl', 'downm', 'downn', 'downo', 'downp', 'downq', 'downr', 'downs', 'downt', 'downu', 'downv', 'downw', 'downx', 'downy', 'downz']\n"
     ]
    }
   ],
   "source": [
    "# Insert Function\n",
    "def insert_letter(word, verbose=False):\n",
    "    split_letter = []\n",
    "    insert_letter = []\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    # Split Word Each Position\n",
    "    split_letter = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    # Insert Letter Each Position\n",
    "    for left, right in split_letter:\n",
    "        for letter in letters:\n",
    "            new_word = left + letter + right\n",
    "            insert_letter.append(new_word)\n",
    "    \n",
    "    if verbose: print(f\"Insert suggestion : {insert_letter}\")\n",
    "    \n",
    "    return insert_letter\n",
    "\n",
    "result = insert_letter('down', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e16fb76a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Swap suggestion : ['odwn', 'dwon', 'donw']\n"
     ]
    }
   ],
   "source": [
    "# Swap Function\n",
    "def swap_letter(word, verbose=False):\n",
    "    split_letter = []\n",
    "    swap_letter = []\n",
    "    \n",
    "    # Split Word Each Position\n",
    "    split_letter = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    # Swap Letter\n",
    "    for left, right in split_letter:\n",
    "        if len(right) > 1:\n",
    "            new_word = left + right[1] + right[0] + right[2:]\n",
    "            swap_letter.append(new_word)\n",
    "            \n",
    "    if verbose: print(f\"Swap suggestion : {swap_letter}\")\n",
    "\n",
    "    return swap_letter\n",
    "\n",
    "result = swap_letter(\"down\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b651ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert suggestion : ['aown', 'bown', 'cown', 'eown', 'fown', 'gown', 'hown', 'iown', 'jown', 'kown', 'lown', 'mown', 'nown', 'oown', 'pown', 'qown', 'rown', 'sown', 'town', 'uown', 'vown', 'wown', 'xown', 'yown', 'zown', 'dawn', 'dbwn', 'dcwn', 'ddwn', 'dewn', 'dfwn', 'dgwn', 'dhwn', 'diwn', 'djwn', 'dkwn', 'dlwn', 'dmwn', 'dnwn', 'dpwn', 'dqwn', 'drwn', 'dswn', 'dtwn', 'duwn', 'dvwn', 'dwwn', 'dxwn', 'dywn', 'dzwn', 'doan', 'dobn', 'docn', 'dodn', 'doen', 'dofn', 'dogn', 'dohn', 'doin', 'dojn', 'dokn', 'doln', 'domn', 'donn', 'doon', 'dopn', 'doqn', 'dorn', 'dosn', 'dotn', 'doun', 'dovn', 'doxn', 'doyn', 'dozn', 'dowa', 'dowb', 'dowc', 'dowd', 'dowe', 'dowf', 'dowg', 'dowh', 'dowi', 'dowj', 'dowk', 'dowl', 'dowm', 'dowo', 'dowp', 'dowq', 'dowr', 'dows', 'dowt', 'dowu', 'dowv', 'doww', 'dowx', 'dowy', 'dowz']\n"
     ]
    }
   ],
   "source": [
    "# Replace Function\n",
    "def replace_letter(word, verbose=False):\n",
    "    split_letter = []\n",
    "    replace_letter = []\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    \n",
    "    # Split Word Each Position\n",
    "    split_letter = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    \n",
    "    # Replace Letter Each Position\n",
    "    for left, right in split_letter:\n",
    "        if right:\n",
    "            for letter in letters:\n",
    "                if right[0] != letter:\n",
    "                    new_word = left + letter + right[1:]\n",
    "                    replace_letter.append(new_word)\n",
    "            \n",
    "    if verbose: print(f\"Insert suggestion : {replace_letter}\")\n",
    "    \n",
    "    return replace_letter\n",
    "\n",
    "result = replace_letter('down', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70dff02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edit results : {'dopn', 'dosn', 'dowx', 'downd', 'wdown', 'domn', 'downw', 'ndown', 'cdown', 'dowl', 'kown', 'dgown', 'dovwn', 'don', 'gown', 'dorn', 'dlown', 'djown', 'uown', 'zdown', 'rdown', 'diown', 'downv', 'dowy', 'doin', 'dcwn', 'dowjn', 'dowfn', 'dvwn', 'doan', 'dnown', 'dowz', 'eown', 'fown', 'downe', 'dowe', 'dofwn', 'dokn', 'hdown', 'doawn', 'mdown', 'downa', 'downc', 'downt', 'donn', 'dotn', 'doyn', 'yown', 'dowr', 'dowen', 'dopwn', 'ldown', 'dmwn', 'downq', 'djwn', 'donwn', 'downr', 'dhwn', 'dowh', 'bdown', 'docwn', 'dowhn', 'doqn', 'dzwn', 'dowt', 'doln', 'kdown', 'qdown', 'doiwn', 'dwon', 'dowi', 'dobn', 'domwn', 'dowkn', 'doswn', 'jown', 'dpown', 'dofn', 'dowsn', 'town', 'downf', 'downy', 'daown', 'lown', 'dxwn', 'dowon', 'dewn', 'ydown', 'dotwn', 'wown', 'dows', 'dqown', 'gdown', 'dowk', 'ddwn', 'dgwn', 'doon', 'dowin', 'jdown', 'dowb', 'dowbn', 'dowrn', 'vown', 'dowj', 'dobwn', 'doown', 'iown', 'doen', 'dowqn', 'ddown', 'nown', 'doun', 'doww', 'doewn', 'dovn', 'dogn', 'dfwn', 'dowg', 'dokwn', 'dorwn', 'own', 'dvown', 'downo', 'mown', 'qown', 'tdown', 'drown', 'doxwn', 'downi', 'dkwn', 'drwn', 'dowm', 'odwn', 'duwn', 'doqwn', 'dwown', 'dowzn', 'dqwn', 'dwwn', 'docn', 'downm', 'dowp', 'downp', 'dowc', 'downl', 'dozn', 'downx', 'downz', 'dnwn', 'dyown', 'edown', 'dowtn', 'dkown', 'dogwn', 'downk', 'downg', 'bown', 'dodn', 'udown', 'dowu', 'dowv', 'dodwn', 'dsown', 'dowun', 'dlwn', 'xown', 'dozwn', 'sown', 'dowdn', 'downs', 'dowf', 'dzown', 'dowcn', 'dohwn', 'dhown', 'dbwn', 'aown', 'dowvn', 'deown', 'rown', 'dojwn', 'fdown', 'dowxn', 'cown', 'dowmn', 'dmown', 'dpwn', 'dswn', 'dowyn', 'doywn', 'dowd', 'doxn', 'dowa', 'dowq', 'pdown', 'vdown', 'duown', 'dfown', 'dowln', 'sdown', 'dow', 'downh', 'adown', 'hown', 'dowgn', 'donw', 'dbown', 'diwn', 'dywn', 'dowwn', 'dohn', 'downn', 'downb', 'oown', 'downj', 'zown', 'dowo', 'idown', 'dtwn', 'dolwn', 'dxown', 'douwn', 'dcown', 'dojn', 'dtown', 'dowan', 'dawn', 'dwn', 'odown', 'xdown', 'pown', 'dowpn', 'downu'}\n"
     ]
    }
   ],
   "source": [
    "# Function\n",
    "def editing_one_letter(word, verbose=False):\n",
    "    letters = word.lower()\n",
    "    suggestions = set(delete_letter(letters) + insert_letter(letters) + swap_letter(letters) + replace_letter(letters))\n",
    "    if verbose: print(f\"Edit results : {suggestions}\")\n",
    "    return suggestions\n",
    "\n",
    "result = editing_one_letter(\"down\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a91389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered word =  nesw \n",
      "suggestions =  {'nesn', 'nest', 'nsw', 'nssw', 'nasw', 'news', 'nes', 'nehw', 'new', 'ness'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('new', 0.001462007029042426),\n",
       " ('news', 0.0002067017207528097),\n",
       " ('nest', 1.6601791422088467e-05)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def autocorrection(word, vocabulary, probabilities, n=3, verbose=False):\n",
    "    suggestions = []\n",
    "    n_best = []\n",
    "    \n",
    "    if word in vocabulary:\n",
    "        suggestions.append(word)\n",
    "    else:\n",
    "        one_edit = editing_one_letter(word)\n",
    "        valid_result = one_edit.intersection(vocabulary)\n",
    "        if valid_result:\n",
    "            suggestions = valid_result\n",
    "            \n",
    "    words_probability = {word: probabilities.get(word, 0) for word in suggestions}\n",
    "    \n",
    "    n_best = sorted(words_probability.items(), key=lambda x: -x[1])[:n]\n",
    "    \n",
    "    if verbose: print(\"entered word = \", word, \"\\nsuggestions = \", suggestions)\n",
    "\n",
    "    return n_best\n",
    "\n",
    "autocorrection(\"nesw\", vocabulary, probabilities, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81acd2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jasonmiracle/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "import spacy\n",
    "\n",
    "# Integrate Pos Tag and Autocorrect\n",
    "def autocorrect_sentence(sentence, vocabulary, probabilities):\n",
    "    tokenized_word = preprocessing(sentence)\n",
    "    corrected = []\n",
    "    \n",
    "    for word in tokenized_word:\n",
    "        suggestions = autocorrection(word, vocabulary, probabilities)\n",
    "        corrected_word = suggestions[0][0] if suggestions else word\n",
    "        corrected.append(corrected_word)\n",
    "        \n",
    "    return corrected\n",
    "\n",
    "def pos_tag_corrected_sentence(sentence, vocabulary, probabilities):\n",
    "    corrected_sentence = autocorrect_sentence(sentence, vocabulary, probabilities)\n",
    "    print(f\"Sentence : {sentence}\\nCorrected : {' '.join(corrected_sentence)}\\n\")\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    pos_tagged = nlp(\" \".join(corrected_sentence))\n",
    "    for token in pos_tagged:\n",
    "        print(token.text, token.pos_, token.tag_)\n",
    "    return pos_tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "300cc711",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence : I am so itred rigth now\n",
      "Corrected : i am so tired right now\n",
      "i PRON PRP\n",
      "am AUX VBP\n",
      "so ADV RB\n",
      "tired ADJ JJ\n",
      "right ADV RB\n",
      "now ADV RB\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I am so itred rigth now\"\n",
    "tagged = pos_tag_corrected_sentence(sentence, vocabulary, probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff354392",
   "metadata": {},
   "source": [
    "# **Evaluation Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44d47ff",
   "metadata": {},
   "source": [
    "# **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d5bd02",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
